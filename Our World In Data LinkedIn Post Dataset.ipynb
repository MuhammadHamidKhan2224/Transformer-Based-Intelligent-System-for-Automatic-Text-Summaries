{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Re Running ALL The Code Again"
      ],
      "metadata": {
        "id": "SXFnHlTMUMFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1 — Install packages"
      ],
      "metadata": {
        "id": "wN6GznLxU34c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install paddlepaddle paddleocr opencv-python-headless pandas numpy pytesseract\n",
        "!apt-get update\n",
        "!apt-get install -y tesseract-ocr\n"
      ],
      "metadata": {
        "id": "E3-aLxjLURAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2 — Upload ZIP file"
      ],
      "metadata": {
        "id": "q6g6LdbLUUFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "eqcEXzbKU63c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3 — Extract ZIP"
      ],
      "metadata": {
        "id": "nhtyH98YU7b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"JS Bank Stock Image.zip\"\n",
        "extract_folder = \"jsbank_images\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_folder)\n",
        "\n",
        "print(\"Top-level files/folders:\")\n",
        "print(os.listdir(extract_folder))\n"
      ],
      "metadata": {
        "id": "w6vBdmZHU7hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4 — List all images inside the inner folder"
      ],
      "metadata": {
        "id": "cxf1taOXU7lE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inner_folder = os.path.join(extract_folder, \"JS Bank Stock Image Dataset YOLO Model\")\n",
        "image_files = os.listdir(inner_folder)\n",
        "print(\"Images found:\")\n",
        "for f in image_files:\n",
        "    print(f)\n"
      ],
      "metadata": {
        "id": "JDM-NldrVKXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5 — Load a sample image"
      ],
      "metadata": {
        "id": "oX6zHVIaU7nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "image_path = os.path.join(inner_folder, \"1763746093882.jpeg\")\n",
        "img = cv2.imread(image_path)\n",
        "\n",
        "if img is None:\n",
        "    print(\"❌ Error: Cannot load the image!\")\n",
        "else:\n",
        "    print(\"✅ Image loaded successfully.\")\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    plt.figure(figsize=(12,10))\n",
        "    plt.imshow(img_rgb)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "I_pB-loUVOj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6 — Run OCR using pytesseract"
      ],
      "metadata": {
        "id": "-y3tOFSPVOxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytesseract\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "text = pytesseract.image_to_string(gray)\n",
        "print(\"---- Extracted Text ----\")\n",
        "print(text)\n"
      ],
      "metadata": {
        "id": "M7Ona7M-VT1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7 — Optional: OCR all images in the folder"
      ],
      "metadata": {
        "id": "fZC7b5ILVO1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_texts = {}\n",
        "\n",
        "for fname in image_files:\n",
        "    path = os.path.join(inner_folder, fname)\n",
        "    img = cv2.imread(path)\n",
        "    if img is None:\n",
        "        continue\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    text = pytesseract.image_to_string(gray)\n",
        "    all_texts[fname] = text\n",
        "for k, v in all_texts.items():\n",
        "    print(f\"--- {k} ---\")\n",
        "    print(v)\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "fH3lq_pxWHfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8 — Clean OCR text and extract numbers"
      ],
      "metadata": {
        "id": "7Anz0xzuVO30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx\n",
        "from docx import Document\n",
        "import re\n",
        "doc = Document()\n",
        "for fname, text in all_texts.items():\n",
        "    cleaned = re.sub(r'[^\\x00-\\x7F]+',' ', text)\n",
        "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
        "    doc.add_heading(f\"Image: {fname}\", level=1)\n",
        "    doc.add_paragraph(cleaned)\n",
        "    doc.add_page_break()\n",
        "doc.save(\"Extracted_Texts.docx\")\n",
        "print(\"✅ DOCX document created: Extracted_Texts.docx\")\n"
      ],
      "metadata": {
        "id": "AL_0JjaTVO6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Textual Analaysis (Natural Language Processing)"
      ],
      "metadata": {
        "id": "mOPydWHqBdLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Extracted Document (TXT or DOCX)"
      ],
      "metadata": {
        "id": "Lk0mBhjNBc1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "\n",
        "doc = Document(\"Extracted_Texts.docx\")\n",
        "\n",
        "full_text = []\n",
        "for para in doc.paragraphs:\n",
        "    full_text.append(para.text)\n",
        "\n",
        "text = \"\\n\".join(full_text)\n",
        "\n",
        "print(text)\n"
      ],
      "metadata": {
        "id": "eouiOUy0Bp11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Cleaning"
      ],
      "metadata": {
        "id": "rLq04j0HBp8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'Image:\\s*\\d+\\.(?:jpg|jpeg|png)', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'[|¬¦]+', ' ', text)\n",
        "    text = re.sub(r\"[^A-Za-z0-9À-ÖØ-öø-ÿ.,!?;:%\\-\\s]\", \" \", text)\n",
        "    text = re.sub(r'[-]{2,}', ' ', text)\n",
        "    text = re.sub(r'[.,!?;:]{2,}', lambda m: m.group(0)[0], text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "cleaned_text = clean_text(text)"
      ],
      "metadata": {
        "id": "bHlaSEelCtNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text"
      ],
      "metadata": {
        "id": "pKCSk9RuBqCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization + Stopword Removal"
      ],
      "metadata": {
        "id": "JdA6-TJ3Bni4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "tokens = word_tokenize(cleaned_text.lower())\n",
        "stops = set(stopwords.words(\"english\"))\n",
        "filtered_tokens = [t for t in tokens if t not in stops and t.isalpha()]"
      ],
      "metadata": {
        "id": "aUnTKYmgC5nP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_tokens\n"
      ],
      "metadata": {
        "id": "Kt-FVIIeC5sW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Named Entity Recognition (NER)"
      ],
      "metadata": {
        "id": "CuK8IAkkC5vJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import defaultdict\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(cleaned_text)\n",
        "\n",
        "entities = defaultdict(set)\n",
        "for ent in doc.ents:\n",
        "    entities[ent.label_].add(ent.text)\n",
        "\n",
        "print(\"\\n\\n==== NAMED ENTITIES GROUPED ====\")\n",
        "for label, vals in entities.items():\n",
        "    print(f\"\\n{label}:\")\n",
        "    for v in sorted(vals):\n",
        "        print(\"  -\", v)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(cleaned_text)\n",
        "\n",
        "entities = defaultdict(set)\n",
        "for ent in doc.ents:\n",
        "    entities[ent.label_].add(ent.text)\n",
        "\n",
        "print(\"\\n\\n==== NAMED ENTITIES GROUPED ====\")\n",
        "for label, vals in entities.items():\n",
        "    print(f\"\\n{label}:\")\n",
        "    for v in sorted(vals):\n",
        "        print(\"  -\", v)"
      ],
      "metadata": {
        "id": "rYayfp7WC5xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment Analysis"
      ],
      "metadata": {
        "id": "-iffCD92MIea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob\n",
        "from textblob import TextBlob\n",
        "sentiment = TextBlob(cleaned_text).sentiment\n",
        "print(\"Polarity:\", sentiment.polarity)\n",
        "print(\"Subjectivity:\", sentiment.subjectivity)"
      ],
      "metadata": {
        "id": "LPeK-fLpC5zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keyword Extraction (TF-IDF)"
      ],
      "metadata": {
        "id": "SzxQrOkZMJGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=20, stop_words='english')\n",
        "tfidf = vectorizer.fit_transform([cleaned_text])\n",
        "keywords = vectorizer.get_feature_names_out()\n",
        "print(\"Top Keywords:\", keywords)"
      ],
      "metadata": {
        "id": "lv4T2pFzMdPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Automatic Text Summarization"
      ],
      "metadata": {
        "id": "hUdFXHFEMdtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text = clean_text(text)\n",
        "\n",
        "!pip install transformers --quiet\n",
        "from transformers import pipeline\n",
        "import math\n",
        "\n",
        "# Initialize summarizer\n",
        "summarizer = pipeline(\n",
        "    \"summarization\",\n",
        "    model=\"sshleifer/distilbart-cnn-12-6\"\n",
        ")\n",
        "\n",
        "# Function to split text into chunks for summarization\n",
        "def chunk_text(text, max_len=1000):\n",
        "    \"\"\"\n",
        "    Split text into chunks of roughly max_len characters at sentence boundaries.\n",
        "    \"\"\"\n",
        "    import nltk\n",
        "    nltk.download(\"punkt\", quiet=True)\n",
        "    from nltk.tokenize import sent_tokenize\n",
        "\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for sent in sentences:\n",
        "        if len(current_chunk) + len(sent) <= max_len:\n",
        "            current_chunk += \" \" + sent\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sent\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "    return chunks\n",
        "\n",
        "# Split cleaned_text into manageable chunks\n",
        "chunks = chunk_text(cleaned_text, max_len=1000)\n",
        "\n",
        "# Generate summary for each chunk\n",
        "summaries = []\n",
        "for chunk in chunks:\n",
        "    summary_chunk = summarizer(\n",
        "        chunk,\n",
        "        max_length=150,\n",
        "        min_length=50,\n",
        "        do_sample=False\n",
        "    )\n",
        "    summaries.append(summary_chunk[0][\"summary_text\"])\n",
        "\n",
        "  # Combine chunk summaries into final summary\n",
        "final_summary = \" \".join(summaries)\n",
        "\n",
        "print(\"\\n=== FINAL SUMMARY ===\\n\")\n",
        "print(final_summary)\n"
      ],
      "metadata": {
        "id": "G84qyx3OMn2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment Analysis with TextBlob and Visualization"
      ],
      "metadata": {
        "id": "vt9puITmOpyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob matplotlib seaborn --quiet\n",
        "from textblob import TextBlob\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sentences = cleaned_text.split('.')\n",
        "polarities = []\n",
        "subjectivities = []\n",
        "for sent in sentences:\n",
        "    blob = TextBlob(sent)\n",
        "    polarities.append(blob.sentiment.polarity)\n",
        "    subjectivities.append(blob.sentiment.subjectivity)"
      ],
      "metadata": {
        "id": "3beFhJ6TMn7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "sns.histplot(polarities, bins=20, kde=True, color='skyblue')\n",
        "plt.title(\"Sentiment Polarity Distribution\")\n",
        "plt.xlabel(\"Polarity (-1 negative → 1 positive)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p7DzxNDMMn9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "sns.histplot(subjectivities, bins=20, kde=True, color='salmon')\n",
        "plt.title(\"Subjectivity Distribution\")\n",
        "plt.xlabel(\"Subjectivity (0 objective → 1 subjective)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "otYU2s3XMdx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Frequency Visualization (Bar Chart / Word Cloud)"
      ],
      "metadata": {
        "id": "T-SBeZb3Md0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordcloud"
      ],
      "metadata": {
        "id": "BaP8V3KLWghW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "word_freq = Counter(filtered_tokens)\n",
        "top_words = word_freq.most_common(100)\n",
        "words, counts = zip(*top_words)\n",
        "plt.figure(figsize=(18,10))\n",
        "sns.barplot(x=list(words), y=list(counts), palette=\"viridis\")\n",
        "plt.title(\"Top 20 Words in Extracted Text\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.xlabel(\"Words\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "TTAbWyIGPiSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Cloud\n",
        "wc = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(word_freq)\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GcDblSVSQA2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Named Entity Recognition (NER) Visualization"
      ],
      "metadata": {
        "id": "EBdtuj-oPj99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy matplotlib seaborn --quiet\n",
        "import spacy\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(cleaned_text)\n",
        "entities = [ent.label_ for ent in doc.ents]\n",
        "entity_counts = Counter(entities)"
      ],
      "metadata": {
        "id": "OSxT-q_eP765"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(20,8))\n",
        "sns.barplot(x=list(entity_counts.keys()), y=list(entity_counts.values()), palette=\"magma\")\n",
        "plt.title(\"Named Entity Types Count\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.xlabel(\"Entity Type\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AiYQkCDYP-OU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combined Sentiment over Text (Line Plot)"
      ],
      "metadata": {
        "id": "OV86V3kiQyUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(polarities, color='blue', marker='o', linestyle='-')\n",
        "plt.title(\"Sentence-wise Polarity Trend\")\n",
        "plt.xlabel(\"Sentence Index\")\n",
        "plt.ylabel(\"Polarity (-1 negative → 1 positive)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tryhKZGNROKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s6tUP-JXRSUj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}